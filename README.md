# agentic-ds-team
Agentic DS Team Experiment

## Overview
This repository provides infrastructure-as-code (IaC) for deploying an agentic Data Science team for fraud detection, supporting both AWS (Bedrock, SageMaker, S3) and GCP (Vertex AI, GCS). The architecture enables full DS lifecycle automation, from data discovery to model deployment, using modular Terraform code.

## Structure
- **AWS Deployment:**
  - `bedrock_agentic_ds_team.tf` (root module)
  - `modules/bedrock_agentic_ds_team/` (AWS-specific modules)
- **GCP Deployment:**
  - `gcp_agentic_ds_team.tf` (root module)
  - `modules/gcp_agentic_ds_team/` (GCP-specific modules)
- **Shared:**
  - `experiment_app.py` (Streamlit front end for experimentation prompt entry)
  - `data/` (fraud datasets in CSV/Parquet)
  - `generated-diagrams/` (architecture diagrams)

## How to Deploy

### AWS
1. Configure AWS credentials (least-privilege IAM user/role recommended).
2. Edit `bedrock_agentic_ds_team.tf` and variables as needed.
3. Run:
   ```zsh
   terraform init
   terraform apply
   ```

### GCP
1. Configure GCP credentials (least-privilege service account recommended).
2. Edit `gcp_agentic_ds_team.tf` and variables as needed.
3. Run:
   ```zsh
   terraform init
   terraform apply
   ```

## Downloading Data

To download and prepare data for the project, use one of the provided scripts in the `/scripts` folder:

### Option 1: Download and Unzip from a URL

1. Open a terminal and navigate to the `scripts` directory:
   ```zsh
   cd scripts
   ```
2. Make the script executable (if not already):
   ```zsh
   chmod +x download_and_prepare_data.sh
   ```
3. Run the script with the URL of the zip file containing your data:
   ```zsh
   ./download_and_prepare_data.sh <zip_url>
   ```
   Replace `<zip_url>` with the actual URL to your data zip file. The script will download, unzip, and copy the data into the `/data` folder at the project root.

### Option 2: Use the `data.sh` Script

If you have a custom data preparation workflow or additional steps, use the `data.sh` script. Make it executable and run it as needed:

```zsh
chmod +x data.sh
./data.sh
```

Check the script for any required arguments or environment variables. This script can be customized for more advanced data preparation tasks.

## Security Best Practices
- S3/GCS buckets are encrypted, versioned, and block public access.
- IAM roles use least privilege and session duration limits.
- SageMaker/Vertex AI endpoints support encryption and tagging.

## Experimentation Front End
- Use `experiment_app.py` with Streamlit to submit experiment instructions to the agentic DS team (update API URL as needed).

## Diagrams
- See `generated-diagrams/` for architecture visualizations.

## Authors
- Generated by GitHub Copilot
